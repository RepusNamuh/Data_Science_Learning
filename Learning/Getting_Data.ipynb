{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"e:\\Data_Science\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../test.txt') as f:\n",
    "    tab_reader = csv.reader(f)\n",
    "    for row in tab_reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        print(date, symbol, closing_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../test.txt') as f:\n",
    "    tab_reader = csv.DictReader(f, fieldnames=['date', 'symbol', 'closing_price'])\n",
    "    for row in tab_reader:\n",
    "        date = row['date']\n",
    "        symbol = row['symbol']\n",
    "        closing_price = float(row['closing_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/joelgrus/data/master/getting-data.html\")\n",
    "\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "first_paragraph = soup.find('p')\n",
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "\n",
    "first_paragraph_id = soup.p.get('id')\n",
    "first_paragraph_id2 = soup.p['id']\n",
    "\n",
    "all_paragraphs = soup.find_all('p')\n",
    "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
    "\n",
    "print(all_paragraphs)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping Tabs on Congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = (\"https://www.house.gov/representatives\")\n",
    "text = requests.get(url).text\n",
    "soup = BeautifulSoup(text, 'html5lib')\n",
    "\n",
    "all_urls = [a['href'] \n",
    "            for a in soup('a')\n",
    "            if a.has_attr('href')]\n",
    "\n",
    "print(len(all_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Must start with http:// or https://\n",
    "# Must end with .house.gov or .house.gov/\n",
    "regex = r\"https?://.*\\.house\\.gov/?$\"\n",
    "\n",
    "\n",
    "good_urls = [url for url in all_urls if re.match(regex, url)]\n",
    "\n",
    "good_urls = list(set(good_urls))\n",
    "print(len(good_urls))\n",
    "\n",
    "html = requests.get('https://jayapal.house.gov').text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "links ={a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Set\n",
    "\n",
    "press_releases: Dict[str, Set[str]] = {}\n",
    "\n",
    "for house_url in good_urls:\n",
    "    html = requests.get(house_url).text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    pr_links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "\n",
    "    press_releases[house_url] = pr_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_mentions(text: str, keyword: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if a <p> inside the text mentions {keyword}\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, 'html5lib')\n",
    "    paragraphs = [p.get_text() for p in soup('p')]\n",
    "\n",
    "    return any(keyword.lower() in paragraph.lower() for paragraph in paragraphs)\n",
    "\n",
    "text = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\"\n",
    "assert paragraph_mentions(text, \"twitter\")\n",
    "assert not paragraph_mentions(text, \"facebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house_url, pr_links in press_releases.items():\n",
    "    for pr_link in pr_links:\n",
    "        url = f\"{house_url}/{pr_link}\"\n",
    "        text = requests.get(url).text\n",
    "\n",
    "        if paragraph_mentions(text, 'data'):\n",
    "            print(f\"{house_url}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "serialised = \"\"\"{\"title\": \"Data Science Book\",\n",
    "                    \"author\": \"Joel Grus\",\n",
    "                    \"publicationYear\": 2019,\n",
    "                    \"topics\": [\"data\", \"science\", \"data science\"]}\"\"\"\n",
    "\n",
    "# parse the JSON to create a Python dict\n",
    "deserialised = json.loads(serialised)\n",
    "assert deserialised[\"publicationYear\"] == 2019\n",
    "assert \"data science\" in deserialised[\"topics\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "github_user = \"RepusNamuh\"\n",
    "endpoint = f\"https://api.github.com/users/{github_user}/repos\"\n",
    "\n",
    "repos = json.loads(requests.get(endpoint).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from dateutil.parser import parse\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "\n",
    "last_5_repositories = sorted(repos, key=lambda r: r[\"pushed_at\"], reverse=True)[:5]\n",
    "\n",
    "last_5_languages = [repo[\"language\"] for repo in last_5_repositories]\n",
    "\n",
    "repos_name = [repo['name'] for repo in repos]\n",
    "print(repos_name)\n",
    "\n",
    "print(last_5_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NS21BKG4cozndD0kRIveN3lIq 5Nc46yo3LxD7OdwJMRQ07YFEH0vRCdrdtt6zNw5kl1rsp7meZe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "CONSUMER_KEY = os.getenv(\"Twitter_CONSUMER_KEY\")\n",
    "CONSUMER_SECRET = os.getenv(\"Twitter_CONSUMER_SECRET\")\n",
    "\n",
    "print(CONSUMER_KEY, CONSUMER_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go visit https://api.twitter.com/oauth/authenticate?oauth_token=6r0fbAAAAAABxEpPAAABk0d9aAw and get the pin code and paste it here\n"
     ]
    }
   ],
   "source": [
    "from math import pi\n",
    "import webbrowser\n",
    "from twython import Twython\n",
    "\n",
    "temp_client = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "temp_creds = temp_client.get_authentication_tokens()\n",
    "url = temp_creds['auth_url']\n",
    "\n",
    "print(f\"go visit {url} and get the pin code and paste it here\")\n",
    "webbrowser.open(url)\n",
    "PIN_CODE = input(\"Enter your pin code: \")\n",
    "\n",
    "auth_client = Twython(CONSUMER_KEY,\n",
    "                        CONSUMER_SECRET,\n",
    "                        temp_creds['oauth_token'],\n",
    "                        temp_creds['oauth_token_secret'])\n",
    "\n",
    "final_step = auth_client.get_authorized_tokens(PIN_CODE)\n",
    "\n",
    "ACCESS_TOKEN = final_step['oauth_token']\n",
    "ACCESS_TOKEN_SECRET = final_step['oauth_token_secret']\n",
    "\n",
    "# And get a new Twython instance using them.\n",
    "twitter = Twython(CONSUMER_KEY,\n",
    "                    CONSUMER_SECRET,\n",
    "                    ACCESS_TOKEN,\n",
    "                    ACCESS_TOKEN_SECRET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import TwythonStreamer\n",
    "\n",
    "# Appending data to a global variable is pretty poor form\n",
    "# but it makes the example much simpler\n",
    "tweets = []\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    def on_success(self, data):\n",
    "        \"\"\"\n",
    "        What do we do when Twitter sends us data?\n",
    "        Here data will be a Python dict representing a tweet\n",
    "        \"\"\"\n",
    "        # We only want to collect English-language tweets\n",
    "        if data.get('lang') == 'en':\n",
    "            tweets.append(data)\n",
    "            print(f\"received tweet #{len(tweets)}\")\n",
    "\n",
    "        # Stop when we've collected enough\n",
    "        if len(tweets) >= 100:\n",
    "            self.disconnect()\n",
    "\n",
    "    def on_error(self, status_code, *data):\n",
    "        print(status_code, data)\n",
    "        self.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 (b'', {'perf': '7402827104', 'cache-control': 'no-cache, no-store, max-age=0', 'content-length': '0', 'x-transaction-id': '353e823a6638589c', 'x-response-time': '93', 'x-connection-hash': '6f8c6598ef6be86b05ed2d47275d05b45a7f1b7e9b655f5cb214ff77275521ed', 'date': 'Wed, 20 Nov 2024 03:05:24 GMT', 'server': 'tsa_m'})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET,\n",
    "                    ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "\n",
    "\n",
    "stream.statuses.filter(track='star rail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(tweets)\n",
    "top_hashtags = Counter(hashtag['text'].lower()\n",
    "                        for tweet in tweets\n",
    "                        for hashtag in tweet[\"entities\"][\"hashtags\"])\n",
    "\n",
    "print(top_hashtags.most_common(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
